Changed directory to /home/jq271/rds/hpc-work/Dissertation/mae.

JobID: 54972123
======
Time: Sun Jun 30 14:21:05 BST 2024
Running on master node: gpu-q-14
Current directory: /home/jq271/rds/hpc-work/Dissertation/mae

Nodes allocated:
================

numtasks=1, numnodes=1, mpi_tasks_per_node=1 (OMP_NUM_THREADS=1)

Executing command:
==================
bash /home/jq271/rds/hpc-work/Dissertation/mae/scripts/finetune.sh

wandb: Currently logged in as: jq271. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.0
wandb: Run data is saved locally in /rds/user/jq271/hpc-work/Dissertation/mae/wandb/run-20240630_142111-r097ju80
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-shadow-15
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jq271/finetune%20mae
wandb: üöÄ View run at https://wandb.ai/jq271/finetune%20mae/runs/r097ju80
| distributed init (rank 0): env://, gpu 0
[14:21:16.490666] job dir: /rds/user/jq271/hpc-work/Dissertation/mae
[14:21:16.490948] Namespace(batch_size=32,
epochs=15,
accum_iter=1,
model='vit_large_patch16',
input_size=224,
drop_path=0.2,
clip_grad=1.0,
weight_decay=0.05,
lr=None,
blr=0.01,
layer_decay=0.75,
min_lr=1e-06,
warmup_epochs=5,
color_jitter=None,
aa='rand-m9-mstd0.5-inc1',
smoothing=0.1,
reprob=0.25,
remode='pixel',
recount=1,
resplit=False,
mixup=0.8,
cutmix=1.0,
cutmix_minmax=None,
mixup_prob=1.0,
mixup_switch_prob=0.5,
mixup_mode='batch',
finetune='/home/jq271/rds/hpc-work/Dissertation/mae/checkpoints/mae_pretrain_vit_large.pth',
global_pool=True,
data_path='/home/jq271/rds/hpc-work/Dissertation/LLaVA/data/VLMPretrain',
nb_classes=1,
output_dir='/home/jq271/rds/hpc-work/Dissertation/mae/finetune_checkpoint',
log_dir='./output_dir',
device='cuda',
seed=0,
resume='',
start_epoch=0,
eval=False,
dist_eval=True,
num_workers=10,
pin_mem=True,
world_size=1,
local_rank=-1,
dist_on_itp=False,
dist_url='env://',
rank=0,
gpu=0,
distributed=True,
dist_backend='nccl')
[14:21:17.198905] GPU Rank:  0
[14:21:17.199269] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x15156b6d52d0>
[14:21:17.214359] Mixup is activated!
[14:21:21.157542] Load pre-trained checkpoint from: /home/jq271/rds/hpc-work/Dissertation/mae/checkpoints/mae_pretrain_vit_large.pth
[14:21:21.266338] _IncompatibleKeys(missing_keys=['head.weight', 'head.bias', 'fc_norm.weight', 'fc_norm.bias'], unexpected_keys=['norm.weight', 'norm.bias'])
[14:21:21.375645] number of params (M): 303.30
[14:21:21.375903] base lr: 1.00e-02
[14:21:21.376016] actual lr: 1.25e-03
[14:21:21.376100] accumulate grad iterations: 1
[14:21:21.376167] effective batch size: 32
[14:21:21.393799] criterion = MSELoss()
[14:21:21.393943] Start training for 15 epochs
[14:21:21.395108] log_dir: ./output_dir
